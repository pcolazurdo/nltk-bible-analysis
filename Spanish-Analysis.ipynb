{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package cess_esp to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Spanish helpers\n",
    "nltk.download('cess_esp')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# !find / -name bible-kjv.txt 2>/dev/null\n",
    "# !ls /root/nltk_data/corpora/gutenberg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "# print(len(stopwords))\n",
    "# new_stopwords = [item.strip() for item in \"a’s, able, about, above, according, accordingly, across, actually, after, afterwards, again, against, ain’t, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, appear, appreciate, appropriate, are, aren’t, around, as, aside, ask, asking, associated, at, available, away, awfully, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, both, brief, but, by, c’mon, c’s, came, can, can’t, cannot, cant, cause, causes, certain, certainly, changes, clearly, co, com, come, comes, concerning, consequently, consider, considering, contain, containing, contains, corresponding, could, couldn’t, course, currently, definitely, described, despite, did, didn’t, different, do, does, doesn’t, doing, don’t, done, down, downwards, during, each, edu, eg, eight, either, else, elsewhere, enough, entirely, especially, et, etc, even, ever, every, everybody, everyone, everything, everywhere, ex, exactly, example, except, far, few, fifth, first, five, followed, following, follows, for, former, formerly, forth, four, from, further, furthermore, get, gets, getting, given, gives, go, goes, going, gone, got, gotten, greetings, had, hadn’t, happens, hardly, has, hasn’t, have, haven’t, having, he, he’s, hello, help, hence, her, here, here’s, hereafter, hereby, herein, hereupon, hers, herself, hi, him, himself, his, hither, hopefully, how, howbeit, however, i’d, i’ll, i’m, i’ve, ie, if, ignored, immediate, in, inasmuch, inc, indeed, indicate, indicated, indicates, inner, insofar, instead, into, inward, is, isn’t, it, it’d, it’ll, it’s, its, itself, just, keep, keeps, kept, know, knows, known, last, lately, later, latter, latterly, least, less, lest, let, let’s, like, liked, likely, little, look, looking, looks, ltd, mainly, many, may, maybe, me, mean, meanwhile, merely, might, more, moreover, most, mostly, much, must, my, myself, name, namely, nd, near, nearly, necessary, need, needs, neither, never, nevertheless, new, next, nine, no, nobody, non, none, noone, nor, normally, not, nothing, novel, now, nowhere, obviously, of, off, often, oh, ok, okay, old, on, once, one, ones, only, onto, or, other, others, otherwise, ought, our, ours, ourselves, out, outside, over, overall, own, particular, particularly, per, perhaps, placed, please, plus, possible, presumably, probably, provides, que, quite, qv, rather, rd, re, really, reasonably, regarding, regardless, regards, relatively, respectively, right, said, same, saw, say, saying, says, second, secondly, see, seeing, seem, seemed, seeming, seems, seen, self, selves, sensible, sent, serious, seriously, seven, several, shall, she, should, shouldn’t, since, six, so, some, somebody, somehow, someone, something, sometime, sometimes, somewhat, somewhere, soon, sorry, specified, specify, specifying, still, sub, such, sup, sure, t’s, take, taken, tell, tends, th, than, thank, thanks, thanx, that, that’s, thats, the, their, theirs, them, themselves, then, thence, there, there’s, thereafter, thereby, therefore, therein, theres, thereupon, these, they, they’d, they’ll, they’re, they’ve, think, third, this, thorough, thoroughly, those, though, three, through, throughout, thru, thus, to, together, too, took, toward, towards, tried, tries, truly, try, trying, twice, two, un, under, unfortunately, unless, unlikely, until, unto, up, upon, us, use, used, useful, uses, using, usually, value, various, very, via, viz, vs, want, wants, was, wasn’t, way, we, we’d, we’ll, we’re, we’ve, welcome, well, went, were, weren’t, what, what’s, whatever, when, whence, whenever, where, where’s, whereafter, whereas, whereby, wherein, whereupon, wherever, whether, which, while, whither, who, who’s, whoever, whole, whom, whose, why, will, willing, wish, with, within, without, won’t, wonder, would, would, wouldn’t, yes, yet, you, you’d, you’ll, you’re, you’ve, your, yours, yourself, yourselves, zero\".split(\n",
    "#     \",\")]\n",
    "\n",
    "# kjv_stopwords = [item.strip() for item in \"thou, thy, ye, thou, thee, \".split(\n",
    "#     \",\")]\n",
    "# for x in new_stopwords:\n",
    "#     stopwords.add(x)\n",
    "# for x in kjv_stopwords:\n",
    "#     stopwords.add(x)\n",
    "# print(len(stopwords))\n",
    "file_content = open(\"./revelation-rvr60.txt\").read()\n",
    "tokens = nltk.word_tokenize(file_content)\n",
    "word_tokens = [word for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dios', 98),\n",
       " ('tierra', 83),\n",
       " ('gran', 65),\n",
       " ('siete', 56),\n",
       " ('cielo', 55),\n",
       " ('ángel', 53),\n",
       " ('trono', 42),\n",
       " ('voz', 41),\n",
       " ('bestia', 36),\n",
       " ('cosas', 32),\n",
       " ('aquí', 31),\n",
       " ('vi', 29),\n",
       " ('nombre', 29),\n",
       " ('ciudad', 29),\n",
       " ('delante', 28),\n",
       " ('siglos', 28),\n",
       " ('mar', 28),\n",
       " ('Cordero', 28),\n",
       " ('fuego', 26),\n",
       " ('mil', 26),\n",
       " ('oí', 24),\n",
       " ('libro', 24),\n",
       " ('diciendo', 24),\n",
       " ('Señor', 23),\n",
       " ('ángeles', 23),\n",
       " ('cuatro', 23),\n",
       " ('doce', 23),\n",
       " ('oro', 22),\n",
       " ('boca', 21),\n",
       " ('obras', 21),\n",
       " ('poder', 21),\n",
       " ('parte', 20),\n",
       " ('sangre', 19),\n",
       " ('dice', 19),\n",
       " ('dijo', 19),\n",
       " ('reyes', 18),\n",
       " ('naciones', 18),\n",
       " ('hombres', 18),\n",
       " ('gloria', 17),\n",
       " ('decía', 17),\n",
       " ('sentado', 17),\n",
       " ('todas', 16),\n",
       " ('pues', 16),\n",
       " ('vida', 16),\n",
       " ('mujer', 16),\n",
       " ('templo', 16),\n",
       " ('cabezas', 16),\n",
       " ('medio', 15),\n",
       " ('muerte', 15),\n",
       " ('tribu', 15)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allWordExceptStopDist = nltk.FreqDist(w for w in word_tokens if w.lower() not in stopwords)    \n",
    "allWordExceptStopDist.most_common()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('know.v.01.know')\n",
      "Lemma('know.v.01.cognize')\n",
      "Lemma('know.v.01.cognise')\n",
      "Lemma('know.v.02.know')\n",
      "Lemma('know.v.03.know')\n",
      "Lemma('know.v.04.know')\n",
      "Lemma('know.v.05.know')\n",
      "Lemma('know.v.05.experience')\n",
      "Lemma('know.v.05.live')\n",
      "Lemma('acknowledge.v.06.acknowledge')\n",
      "Lemma('acknowledge.v.06.recognize')\n",
      "Lemma('acknowledge.v.06.recognise')\n",
      "Lemma('acknowledge.v.06.know')\n",
      "Lemma('know.v.07.know')\n",
      "Lemma('sleep_together.v.01.sleep_together')\n",
      "Lemma('sleep_together.v.01.roll_in_the_hay')\n",
      "Lemma('sleep_together.v.01.love')\n",
      "Lemma('sleep_together.v.01.make_out')\n",
      "Lemma('sleep_together.v.01.make_love')\n",
      "Lemma('sleep_together.v.01.sleep_with')\n",
      "Lemma('sleep_together.v.01.get_laid')\n",
      "Lemma('sleep_together.v.01.have_sex')\n",
      "Lemma('sleep_together.v.01.know')\n",
      "Lemma('sleep_together.v.01.do_it')\n",
      "Lemma('sleep_together.v.01.be_intimate')\n",
      "Lemma('sleep_together.v.01.have_intercourse')\n",
      "Lemma('sleep_together.v.01.have_it_away')\n",
      "Lemma('sleep_together.v.01.have_it_off')\n",
      "Lemma('sleep_together.v.01.screw')\n",
      "Lemma('sleep_together.v.01.fuck')\n",
      "Lemma('sleep_together.v.01.jazz')\n",
      "Lemma('sleep_together.v.01.eff')\n",
      "Lemma('sleep_together.v.01.hump')\n",
      "Lemma('sleep_together.v.01.lie_with')\n",
      "Lemma('sleep_together.v.01.bed')\n",
      "Lemma('sleep_together.v.01.have_a_go_at_it')\n",
      "Lemma('sleep_together.v.01.bang')\n",
      "Lemma('sleep_together.v.01.get_it_on')\n",
      "Lemma('sleep_together.v.01.bonk')\n",
      "Lemma('know.v.09.know')\n",
      "Lemma('know.v.10.know')\n",
      "Lemma('know.v.11.know')\n",
      "Lemma('known.a.01.known')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"known\"):\n",
    "    for l in syn.lemmas():\n",
    "        print (l)\n",
    "#         synonyms.append(l.name())\n",
    "#         if l.antonyms():\n",
    "#             antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "# print(set(synonyms))\n",
    "# print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nltk.download('averaged_perceptron_tagger')\n",
    "# text = nltk.word_tokenize(file_content)\n",
    "# pos_tagged = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FROM: https://github.com/alvations/spaghetti-tagger\n",
    "\n",
    "#-*- coding: utf8 -*-\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "from nltk import UnigramTagger, BigramTagger\n",
    "from nltk.corpus import cess_esp as cess\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "def load_tagger(filename):\n",
    "    \"\"\" Function to load tagger. \"\"\"\n",
    "    with open(filename,'rb') as fin:\n",
    "        tagger = pickle.load(fin)\n",
    "    return tagger\n",
    "\n",
    "def save_tagger(filename, tagger):\n",
    "    \"\"\" Function to save tagger. \"\"\"\n",
    "    with open(filename, 'wb') as fout:\n",
    "        pickle.dump(tagger, fout)\n",
    "\n",
    "def train_tagger(corpus_name, corpus):\n",
    "    \"\"\" Function to train tagger. \"\"\"\n",
    "    # Training UnigramTagger.\n",
    "    uni_tag = UnigramTagger(corpus)\n",
    "    save_tagger('{}_unigram.tagger'.format(corpus_name), uni_tag)\n",
    "    # Training BigramTagger.\n",
    "    bi_tag = BigramTagger(corpus, backoff=uni_tag)\n",
    "    save_tagger('{}_bigram.tagger'.format(corpus_name), bi_tag)\n",
    "    _msg = str(\"Tagger trained with {} using \"\n",
    "            \"UnigramTagger and BigramTagger.\").format(corpus_name)\n",
    "    print (_msg, file=sys.stderr)\n",
    "\n",
    "def unchunk(corpus): \n",
    "    \"\"\" Function to unchunk corpus. \"\"\"\n",
    "    nomwe_corpus = []\n",
    "    for i in corpus:\n",
    "        nomwe = \" \".join([j[0].replace(\"_\",\" \") for j in i])\n",
    "        nomwe_corpus.append(nomwe.split())\n",
    "    return nomwe_corpus\n",
    "\n",
    "\n",
    "class CESSTagger():\n",
    "    def __init__(self,use_mwe=False):\n",
    "        self.use_mwe = use_mwe\n",
    "        # Train tagger if it's used for the first time.\n",
    "        try:\n",
    "            load_tagger('cess_unigram.tagger').tag(['estoy'])\n",
    "            load_tagger('cess_bigram.tagger').tag(['estoy'])\n",
    "        except IOError:\n",
    "            print (\"*** First-time use of cess tagger ***\", file=sys.stderr)\n",
    "            print (\"Training tagger ...\", file=sys.stderr)\n",
    "            # Load CESS corpus.\n",
    "            cess_sents = cess.tagged_sents()\n",
    "            train_tagger('cess',cess_sents)\n",
    "            # Trains the tagger with no MWE.\n",
    "            cess_nomwe = unchunk(cess.tagged_sents())\n",
    "            tagged_cess_nomwe = pos_tag_sents(cess_nomwe, False)\n",
    "            train_tagger('cess_nomwe',tagged_cess_nomwe)\n",
    "        # Load tagger.\n",
    "        _mwe_option_name = \"_nomwe_\" if self.use_mwe == True else \"_\"\n",
    "        self.uni = load_tagger('cess{}unigram.tagger'.format(_mwe_option_name))\n",
    "        self.bi = load_tagger('cess{}bigram.tagger'.format(_mwe_option_name))\n",
    "\n",
    "\n",
    "def pos_tag(tokens, use_mwe=False):\n",
    "    tagger = CESSTagger(use_mwe)\n",
    "    return tagger.uni.tag(tokens)\n",
    "    \n",
    "def pos_tag_sents(sentences, use_mwe=False):\n",
    "    tagger = CESSTagger(use_mwe)\n",
    "    return tagger.uni.tag_sents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(file_content)\n",
    "pos_tagged = pos_tag(text)\n",
    "# spa_tagger = CESSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/software/spanish-faq.html#example\n",
    "verbos = [(sub[0]) for sub in pos_tagged if (sub[1] and sub[1][0] == 'v')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'declaró': 1,\n",
       " 'lee': 1,\n",
       " 'oyen': 1,\n",
       " 'verá': 1,\n",
       " 'ves': 1,\n",
       " 'vuelto': 1,\n",
       " 'llegaba': 1,\n",
       " 'caí': 1,\n",
       " 'estuve': 1,\n",
       " 'anda': 1,\n",
       " 'puedes': 1,\n",
       " 'soportar': 1,\n",
       " 'sufrido': 1,\n",
       " 'tenido': 1,\n",
       " 'trabajado': 1,\n",
       " 'dejado': 1,\n",
       " 'estuvo': 1,\n",
       " 'vas': 1,\n",
       " 'sufrirá': 1,\n",
       " 'poner': 1,\n",
       " 'cometer': 1,\n",
       " 'escondido': 1,\n",
       " 'conoce': 1,\n",
       " 'arrepentirse': 1,\n",
       " 'llaman': 1,\n",
       " 'digo': 1,\n",
       " 'afirma': 1,\n",
       " 'puede': 1,\n",
       " 'vengan': 1,\n",
       " 'probar': 1,\n",
       " 'desciende': 1,\n",
       " 'seas': 1,\n",
       " 'descubra': 1,\n",
       " 'amo': 1,\n",
       " 'sé': 1,\n",
       " 'llamo': 1,\n",
       " 'siente': 1,\n",
       " 'hablando': 1,\n",
       " 'volando': 1,\n",
       " 'dan': 1,\n",
       " 'recibir': 1,\n",
       " 'existen': 1,\n",
       " 'leerlo': 1,\n",
       " 'creado': 1,\n",
       " 'venciendo': 1,\n",
       " 'vencer': 1,\n",
       " 'quitar': 1,\n",
       " 'seguía': 1,\n",
       " 'descansasen': 1,\n",
       " 'volvió': 1,\n",
       " 'deja': 1,\n",
       " 'caer': 1,\n",
       " 'contar': 1,\n",
       " 'pertenece': 1,\n",
       " 'dije': 1,\n",
       " 'salido': 1,\n",
       " 'sirven': 1,\n",
       " 'caerá': 1,\n",
       " 'guiará': 1,\n",
       " 'dispusieron': 1,\n",
       " 'destruida': 1,\n",
       " 'hicieron': 1,\n",
       " 'hubiese': 1,\n",
       " 'sonar': 1,\n",
       " 'mandó': 1,\n",
       " 'tuviesen': 1,\n",
       " 'corriendo': 1,\n",
       " 'dañar': 1,\n",
       " 'vienen': 1,\n",
       " 'dejaron': 1,\n",
       " 'pueden': 1,\n",
       " 'oír': 1,\n",
       " 'andar': 1,\n",
       " 'emitieron': 1,\n",
       " 'emitido': 1,\n",
       " 'iba': 1,\n",
       " 'escribir': 1,\n",
       " 'dicho': 1,\n",
       " 'levantó': 1,\n",
       " 'creó': 1,\n",
       " 'sería': 1,\n",
       " 'tocar': 1,\n",
       " 'anunció': 1,\n",
       " 'fui': 1,\n",
       " 'diciéndole': 1,\n",
       " 'Es': 1,\n",
       " 'mide': 1,\n",
       " 'entregado': 1,\n",
       " 'devora': 1,\n",
       " 'hacerles': 1,\n",
       " 'convertirlas': 1,\n",
       " 'quieran': 1,\n",
       " 'hayan': 1,\n",
       " 'acabado': 1,\n",
       " 'hará': 1,\n",
       " 'permitirán': 1,\n",
       " 'entró': 1,\n",
       " 'levantaron': 1,\n",
       " 'derrumbó': 1,\n",
       " 'damos': 1,\n",
       " 'temen': 1,\n",
       " 'destruir': 1,\n",
       " 'veía': 1,\n",
       " 'estando': 1,\n",
       " 'apareció': 1,\n",
       " 'arrastraba': 1,\n",
       " 'arrojados': 1,\n",
       " 'descendido': 1,\n",
       " 'sabiendo': 1,\n",
       " 'fuese': 1,\n",
       " 'ayudó': 1,\n",
       " 'echado': 1,\n",
       " 'luchar': 1,\n",
       " 'actuar': 1,\n",
       " 'vencerlos': 1,\n",
       " 'lleva': 1,\n",
       " 'mata': 1,\n",
       " 'ejerce': 1,\n",
       " 'permitido': 1,\n",
       " 'hagan': 1,\n",
       " 'hiciese': 1,\n",
       " 'hacía': 1,\n",
       " 'pudiese': 1,\n",
       " 'comprar': 1,\n",
       " 'vender': 1,\n",
       " 'cuente': 1,\n",
       " 'aprender': 1,\n",
       " 'adora': 1,\n",
       " 'vaciado': 1,\n",
       " 'reciba': 1,\n",
       " 'mueren': 1,\n",
       " 'metió': 1,\n",
       " 'Salió': 1,\n",
       " 'llamó': 1,\n",
       " 'echó': 1,\n",
       " 'pisado': 1,\n",
       " 'alcanzado': 1,\n",
       " 'manifestado': 1,\n",
       " 'hubiesen': 1,\n",
       " 'cumplido': 1,\n",
       " 'Fue': 1,\n",
       " 'convirtieron': 1,\n",
       " 'merecen': 1,\n",
       " 'quemar': 1,\n",
       " 'quemaron': 1,\n",
       " 'cubrió': 1,\n",
       " 'estuviese': 1,\n",
       " 'salir': 1,\n",
       " 'van': 1,\n",
       " 'reunió': 1,\n",
       " 'hallados': 1,\n",
       " 'trae': 1,\n",
       " 'ir': 1,\n",
       " 'tenga': 1,\n",
       " 'dure': 1,\n",
       " 'recibirán': 1,\n",
       " 'quiso': 1,\n",
       " 'ponerse': 1,\n",
       " 'acordado': 1,\n",
       " 'preparó': 1,\n",
       " 'viajan': 1,\n",
       " 'trabajan': 1,\n",
       " 'echaron': 1,\n",
       " 'dijeron': 1,\n",
       " 'llamaba': 1,\n",
       " 'conocía': 1,\n",
       " 'Estaba': 1,\n",
       " 'seguían': 1,\n",
       " 'pisa': 1,\n",
       " 'vuelan': 1,\n",
       " 'cumplidos': 1,\n",
       " 'desatado': 1,\n",
       " 'vivieron': 1,\n",
       " 'volvieron': 1,\n",
       " 'vivir': 1,\n",
       " 'cumplieron': 1,\n",
       " 'descendió': 1,\n",
       " 'huyeron': 1,\n",
       " 'encontró': 1,\n",
       " 'entregó': 1,\n",
       " 'existía': 1,\n",
       " 'hago': 1,\n",
       " 'seré': 1,\n",
       " 'halla': 1,\n",
       " 'entrará': 1,\n",
       " 'produce': 1,\n",
       " 'dando': 1,\n",
       " 'servirán': 1,\n",
       " 'mostrar': 1,\n",
       " 'oyó': 1,\n",
       " 'mostraba': 1,\n",
       " 'lavan': 1,\n",
       " 'tener': 1,\n",
       " 'diga': 1,\n",
       " 'quiera': 1,\n",
       " 'quitará': 1,\n",
       " 'da': 1,\n",
       " 'ven': 1,\n",
       " 'deben': 2,\n",
       " 'suceder': 2,\n",
       " 'viene': 2,\n",
       " 'harán': 2,\n",
       " 'ver': 2,\n",
       " 'vivió': 2,\n",
       " 'Sé': 2,\n",
       " 'negado': 2,\n",
       " 'recibe': 2,\n",
       " 'cierra': 2,\n",
       " 'cerrar': 2,\n",
       " 'guardado': 2,\n",
       " 'tome': 2,\n",
       " 'saldrá': 2,\n",
       " 'estoy': 2,\n",
       " 'tomado': 2,\n",
       " 'tomar': 2,\n",
       " 'hay': 2,\n",
       " 'cayeron': 2,\n",
       " 'podrá': 2,\n",
       " 'subía': 2,\n",
       " 'sean': 2,\n",
       " 'tendrán': 2,\n",
       " 'paró': 2,\n",
       " 'subió': 2,\n",
       " 'quemó': 2,\n",
       " 'ardiendo': 2,\n",
       " 'murió': 2,\n",
       " 'murieron': 2,\n",
       " 'volar': 2,\n",
       " 'salieron': 2,\n",
       " 'pasó': 2,\n",
       " 'estarán': 2,\n",
       " 'verán': 2,\n",
       " 'vieron': 2,\n",
       " 'subieron': 2,\n",
       " 'eras': 2,\n",
       " 'juzgar': 2,\n",
       " 'huyó': 2,\n",
       " 'engaña': 2,\n",
       " 'arrojado': 2,\n",
       " 'vio': 2,\n",
       " 'permitió': 2,\n",
       " 'va': 2,\n",
       " 'siguen': 2,\n",
       " 'siguió': 2,\n",
       " 'Ha': 2,\n",
       " 'beber': 2,\n",
       " 'teniendo': 2,\n",
       " 'vendrán': 2,\n",
       " 'entrar': 2,\n",
       " 'juzgado': 2,\n",
       " 'darle': 2,\n",
       " 'hacen': 2,\n",
       " 'vean': 2,\n",
       " 'llevó': 2,\n",
       " 'viendo': 2,\n",
       " 'sienta': 2,\n",
       " 'cumplan': 2,\n",
       " 'vivido': 2,\n",
       " 'juzga': 2,\n",
       " 'llorando': 2,\n",
       " 'juzgados': 2,\n",
       " 'pasaron': 2,\n",
       " 'mostró': 2,\n",
       " 'inscritos': 2,\n",
       " 'hizo': 3,\n",
       " 'Tenía': 3,\n",
       " 'hallado': 3,\n",
       " 'quiere': 3,\n",
       " 'venga': 3,\n",
       " 'recibido': 3,\n",
       " 'morir': 3,\n",
       " 'abre': 3,\n",
       " 'oye': 3,\n",
       " 'vencido': 3,\n",
       " 'salían': 3,\n",
       " 'decir': 3,\n",
       " 'tomó': 3,\n",
       " 'dada': 3,\n",
       " 'matar': 3,\n",
       " 'sale': 3,\n",
       " 'llenó': 3,\n",
       " 'convirtió': 3,\n",
       " 'medir': 3,\n",
       " 'debe': 3,\n",
       " 'sube': 3,\n",
       " 'dar': 3,\n",
       " 'preparado': 3,\n",
       " 'halló': 3,\n",
       " 'subir': 3,\n",
       " 'recibieron': 3,\n",
       " 'estará': 3,\n",
       " 'guardan': 4,\n",
       " 'hablaba': 4,\n",
       " 'puso': 4,\n",
       " 'dicen': 4,\n",
       " 'comer': 4,\n",
       " 'escribe': 4,\n",
       " 'podía': 4,\n",
       " 'mira': 4,\n",
       " 'hacer': 4,\n",
       " 'habló': 4,\n",
       " 'descender': 4,\n",
       " 'venir': 5,\n",
       " 'tengo': 5,\n",
       " 'serán': 5,\n",
       " 'vive': 5,\n",
       " 'abrir': 5,\n",
       " 'hecho': 5,\n",
       " 'llegado': 5,\n",
       " 'venido': 5,\n",
       " 'cayó': 5,\n",
       " 'hace': 5,\n",
       " 'habrá': 5,\n",
       " 'sea': 6,\n",
       " 'llama': 6,\n",
       " 'caído': 6,\n",
       " 'dieron': 6,\n",
       " 'arrojó': 6,\n",
       " 'conozco': 7,\n",
       " 'tienes': 7,\n",
       " 'salió': 7,\n",
       " 'habían': 7,\n",
       " 'tocó': 7,\n",
       " 'derramó': 7,\n",
       " 'eran': 8,\n",
       " 'visto': 9,\n",
       " 'tienen': 9,\n",
       " 'será': 9,\n",
       " 'sido': 9,\n",
       " 'dado': 10,\n",
       " 'daré': 10,\n",
       " 'eres': 10,\n",
       " 'abrió': 10,\n",
       " 'dio': 11,\n",
       " 'estaban': 11,\n",
       " 'hubo': 11,\n",
       " 'He': 12,\n",
       " 'soy': 12,\n",
       " 'ser': 13,\n",
       " 'vino': 13,\n",
       " 'había': 15,\n",
       " 'fueron': 15,\n",
       " 'decía': 17,\n",
       " 'tenían': 18,\n",
       " 'dice': 19,\n",
       " 'dijo': 19,\n",
       " 'están': 20,\n",
       " 'tenía': 20,\n",
       " 'estaba': 22,\n",
       " 'está': 23,\n",
       " 'tiene': 24,\n",
       " 'diciendo': 24,\n",
       " 'he': 26,\n",
       " 'era': 27,\n",
       " 'has': 27,\n",
       " 'han': 27,\n",
       " 'vi': 29,\n",
       " 'fue': 32,\n",
       " 'ha': 35,\n",
       " 'son': 35,\n",
       " 'es': 39}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(list(zip(wordlist,wordfreq)))\n",
    "\n",
    "dict(sorted(wordListToFreqDict(verbos).items(), key=lambda item: item[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
